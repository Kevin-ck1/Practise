{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpeR9aETcRDP"
   },
   "source": [
    "<font color=\"blue\">To use this notebook on Google Colaboratory, you will need to make a copy of it. Go to **File** > **Save a Copy in Drive**. You can then use the new copy that will appear in the new tab.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e13tRpKcJHm"
   },
   "source": [
    "# AfterWork Data Science: Classification Analysis with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA_HMQbPHWQJ"
   },
   "source": [
    "## Importing the Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moay-lWZm7z1"
   },
   "outputs": [],
   "source": [
    "# We will start by running this cell which will import the necessary libraries\n",
    "# ---\n",
    "# \n",
    "import pandas as pd                # Pandas for data manipulation\n",
    "import numpy as np                 # Numpy for scientific computations\n",
    "import matplotlib.pyplot as plt    # Matplotlib for visualisation - We might not use it but just incase you decide to \n",
    "%matplotlib inline                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPpfqZYzrKvs"
   },
   "source": [
    "## Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6pUHkWwrI8l"
   },
   "outputs": [],
   "source": [
    "# Example \n",
    "# ---\n",
    "# Question: Will John, 40 years old with a salary of 2500 buy a car?\n",
    "# ---\n",
    "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eagUqvVmeRTx"
   },
   "source": [
    "#### Data Importation and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1pbsTYuPOLe"
   },
   "outputs": [],
   "source": [
    "# Loading and previewing our dataset\n",
    "# ---\n",
    "# \n",
    "social_df = pd.read_csv('http://bit.ly/SocialNetworkAdsDataset')\n",
    "social_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPZkxTRrdQaC"
   },
   "outputs": [],
   "source": [
    "# Determining the size of our dataset\n",
    "# (records, columns)\n",
    "# ---\n",
    "# \n",
    "social_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz6qTiItChtg"
   },
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtaBMmZBCkW1"
   },
   "outputs": [],
   "source": [
    "# Normally during this stage we would perform quite a number of \n",
    "# procedures, but because our focus is only on learning about the \n",
    "# different modeling algorithms, we will only perform once \n",
    "# essential step in ot dataset. We will perform encoding,\n",
    "# which will help us transform our categorical values in our \n",
    "# dataset into numerical values. \n",
    "# Lets see what happens when we encode the gender variable \n",
    "# to have only numerical values. \n",
    "# ---\n",
    "#\n",
    "social_df[\"Gender\"] = np.where(social_df[\"Gender\"].str.contains(\"Male\", \"Female\"), 1, 0)\n",
    "social_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8NJS6N6eU7j"
   },
   "source": [
    "#### Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybg1VYOGSXbr"
   },
   "outputs": [],
   "source": [
    "# Preparing our dataset for training\n",
    "# ---\n",
    "# We first divide our data into attributes and labels:\n",
    "# You can think of this as splitting our dataset into independent and independent variables \n",
    "# where Age and EstimatedSalary are the independent variables and Purchased are the dependent/label variable.\n",
    "# ---\n",
    "# \n",
    "X = social_df.iloc[:, [1, 2 ,3]].values  # Independent/predictor variables\n",
    "y = social_df.iloc[:, 4].values          # Dependent/label variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFyZcCeVScEp"
   },
   "outputs": [],
   "source": [
    "# Splitting the dataset into a training set and test set\n",
    "# ---\n",
    "# We will split our dataset into training data and test data. \n",
    "# Training data will be used to train our logistic model and test data will be used to validate our model\n",
    "# Because weâ€™ll use sklearn to split our data, we will import train_test_split from sklearn.model_selection\n",
    "# ---\n",
    "# \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sgn73UwVS9hf"
   },
   "outputs": [],
   "source": [
    "# Feature Scaling / Normalisation\n",
    "# ---\n",
    "# We then perform feature scaling / normalisation to scale our data between 0 and 1 so as to get better accuracy.\n",
    "# Here, scaling is important because there is a huge difference between Age and EstimatedSalary.\n",
    "# In addition, this would also reduce redundacy in our dataset. \n",
    "# ---\n",
    "#  \n",
    "\n",
    "# We perform normalisation\n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "norm = MinMaxScaler().fit(X_train) \n",
    "X_train = norm.transform(X_train) \n",
    "X_test = norm.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cDNlsiGTaRF"
   },
   "outputs": [],
   "source": [
    "# In this example, because we will be comparing how \n",
    "# the different classification algorithms will perform, \n",
    "# we import our classifiers as shown below.\n",
    "# ---\n",
    "#\n",
    "from sklearn.linear_model import LogisticRegression # Logistic Regression Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier     # Decision Tree Classifier\n",
    "from sklearn.svm import SVC                         # SVM Classifier\n",
    "from sklearn.naive_bayes import GaussianNB          # Naive Bayes Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier  # KNN Classifier\n",
    "\n",
    "# Below, we make an instance classifier of the object LogisticRegression, \n",
    "# DecisionTreeClassifier, SVC, GaussianNB, KNeighborsClassifier, GaussianNB.\n",
    "# As we will get to see, each of the classifiers take different parameters.\n",
    "# ---\n",
    "# \n",
    "logistic_classifier = LogisticRegression()\n",
    "decision_classifier = DecisionTreeClassifier()\n",
    "svm_classifier = SVC()\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "naive_classifier = GaussianNB()\n",
    "\n",
    "# Now using these classifiers to fit our data, X_train and y_train.\n",
    "# By fitting we mean we train our classifiers based on the train dataset.\n",
    "# ---\n",
    "# Upon running this cell, we should have classifiers that can predict \n",
    "# whether a person will buy a car or not.\n",
    "# ---\n",
    "# Don't worry about the output, we get GaussianNB because our Naive Bayes classifier\n",
    "# is the last one to be built.\n",
    "# ---\n",
    "#\n",
    "logistic_classifier.fit(X_train, y_train)\n",
    "decision_classifier.fit(X_train, y_train)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "naive_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAx-7zU7Tyrt"
   },
   "outputs": [],
   "source": [
    "# We now predict the test set results. \n",
    "# This will help us determine whether our classifiers made the correct predictions.\n",
    "# ---\n",
    "# No expected output here.\n",
    "# ---\n",
    "logistic_y_prediction = logistic_classifier.predict(X_test) \n",
    "decision_y_prediction = decision_classifier.predict(X_test) \n",
    "svm_y_prediction = svm_classifier.predict(X_test) \n",
    "knn_y_prediction = knn_classifier.predict(X_test) \n",
    "naive_y_prediction = naive_classifier.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rG_q-tVaUFxy"
   },
   "outputs": [],
   "source": [
    "# We then import evaluation metrics to determine the accuracy of classifiers\n",
    "# ---\n",
    "# \n",
    "from sklearn.metrics import classification_report, accuracy_score \n",
    "\n",
    "# The accuracy score - is the simplest way to evaluate \n",
    "# However, we note not for a highly imbalance dataset. \n",
    "# By imbalanced we mean that our original dataset would\n",
    "# need to have an equal no's of 1 and 0's\n",
    "# ---\n",
    "print(accuracy_score(logistic_y_prediction, y_test))\n",
    "print(accuracy_score(decision_y_prediction, y_test))\n",
    "print(accuracy_score(svm_y_prediction, y_test))\n",
    "print(accuracy_score(knn_y_prediction, y_test))\n",
    "print(accuracy_score(naive_y_prediction, y_test))\n",
    "\n",
    "# From the accuracy scores we get 90%, 90%, 93%, 93% & 91% respectively.\n",
    "# The most accurate classifier being SVM & KNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4wgNdhhrdVK"
   },
   "outputs": [],
   "source": [
    "# We now print the classification report, \n",
    "# which is more reliable for a highly imbalanced dataset. \n",
    "# We use the precision values which give us accuracy values.\n",
    "# \n",
    "# ---\n",
    "# The precision will be \"how many are correctly classified among that class\".\n",
    "# The recall means \"how many of this class you find over the whole number of element of this class\".\n",
    "# The f1-score is the harmonic mean between precision & recall.\n",
    "# The support is the number of occurence of the given class in your dataset.\n",
    "# ---\n",
    "# \n",
    "print('Logistic classifier:')\n",
    "print(classification_report(y_test, logistic_y_prediction))\n",
    "\n",
    "print('Decision Tree classifier:')\n",
    "print(classification_report(y_test, decision_y_prediction))\n",
    "\n",
    "print('SVM Classifier:')\n",
    "print(classification_report(y_test, svm_y_prediction))\n",
    "\n",
    "print('KNN Classifier:')\n",
    "print(classification_report(y_test, knn_y_prediction))\n",
    "\n",
    "print('Naive Bayes Classifier:')\n",
    "print(classification_report(y_test, naive_y_prediction)) \n",
    "\n",
    "# From our classification report,\n",
    "# Our support tells us that our dataset is highly imbalanced i.e. 63 0's and 32 1's.\n",
    "# From the weighted avg which takes into account of our imbalanced\n",
    "# dataset we get 90%, 90%, 93%, 93%, 91%.\n",
    "# Still, the most accurate classifiers being SVM and KNN. \n",
    "# We can then further perform model opmization techiniques i.e. \n",
    "# data cleaning, feature engineering, checking for model assumptions, etc. \n",
    "# to further get the best classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_a-Rfv_ZfYQ"
   },
   "outputs": [],
   "source": [
    "# We can also use a confusion matrix to determine the accuracy of our model\n",
    "# This will give more details about the model performance.\n",
    "# ---\n",
    "# A confusion matrix is a summary of prediction results on a classification problem. \n",
    "# The number of correct and incorrect predictions are summarized with count values and broken down by each class.\n",
    "# The confusion matrix shows the ways in which your classification model is confused when it makes predictions. \n",
    "# It gives us insight not only into the errors being made by a classifier but more importantly the types of errors \n",
    "# that are being made. The number of correct predictions for each class run on the diagonal \n",
    "# from top-left to bottom-right.\n",
    "# ---\n",
    "# In the confusion matrix below we get:\n",
    "# [[66  7]\n",
    "# [2 25]]\n",
    "# This can be interpreted by:\n",
    "# ---\n",
    "# [[TP, TN]\n",
    "# [FP, FN]]\n",
    "# ---\n",
    "# True Positive (TP) : Observation is positive, and is predicted to be positive.\n",
    "# False Negative (FN) : Observation is positive, but is predicted negative.\n",
    "# True Negative (TN) : Observation is negative, and is predicted to be negative.\n",
    "# False Positive (FP) : Observation is negative, but is predicted positive.\n",
    "# ---\n",
    "# \n",
    "from sklearn.metrics import confusion_matrix \n",
    " \n",
    "print('Logistic Regression classifier:')\n",
    "print(confusion_matrix(logistic_y_prediction, y_test))\n",
    "\n",
    "print('Decision Tree classifier:')\n",
    "print(confusion_matrix(decision_y_prediction, y_test))\n",
    "\n",
    "print('KNN Classifier:')\n",
    "print(confusion_matrix(knn_y_prediction, y_test))\n",
    "\n",
    "print('SVM classifier:')\n",
    "print(confusion_matrix(svm_y_prediction, y_test))\n",
    "\n",
    "print('Naive Bayes classifier:')\n",
    "print(confusion_matrix(naive_y_prediction, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnqqUOET9QLu"
   },
   "outputs": [],
   "source": [
    "# Answering our question\n",
    "# ---\n",
    "# We then make a new prediction & compare results.\n",
    "# Note that we would only use the best optimized classifier for this case.\n",
    "# ---\n",
    "# Predict whether John, 60 years old with a salary of 2500 will buy a car or not?\n",
    "# ---\n",
    "# Dataset limitation: This is not a practical dataset, thus dataset will lack essential features/variables.\n",
    "# In a real case scenario, we would work with may kinds of features that require transformation\n",
    "# i.e. data cleaning, feature engineering, etc.\n",
    "# ---\n",
    "# 1: Represents Male\n",
    "# 60: Represents age\n",
    "# 2500: Represents Salary\n",
    "# ---\n",
    "# \n",
    "new_case = [[0,\t60, 2500]]\n",
    "\n",
    "# We will need to transform our new case\n",
    "new_case = norm.transform(new_case)  \n",
    "\n",
    "print('Logistic Regression classifier', logistic_classifier.predict(new_case))\n",
    "print('Decision Tree classifier:', decision_classifier.predict(new_case))\n",
    "print('SVM classifier:', svm_classifier.predict(new_case))\n",
    "print('KNN classifier:', knn_classifier.predict(new_case)) \n",
    "print('Naive Bayes classifier:', naive_classifier.predict(new_case))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBSYp3fVIupH"
   },
   "source": [
    "##<font color=\"green\">Challenges</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uebT2koxr4V4"
   },
   "source": [
    "###<font color=\"green\">Challenge 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TVcDuh88sFLE"
   },
   "outputs": [],
   "source": [
    "# Challenge 1\n",
    "# ---\n",
    "# Question: As a Reseacher at KEMRI you are performing research on diabetes.\n",
    "# Create the a classifier to determine whether a person has diabetes or not\n",
    "# from the given the following sample dataset.\n",
    "# ---\n",
    "# Dataset url = http://bit.ly/ADiabetesDataset\n",
    "# ---\n",
    "# OUR CODE GOES BELOW\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-eE6btO3KKM"
   },
   "source": [
    "###<font color=\"green\">Challenge 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9b-Q2CMI2wg1"
   },
   "outputs": [],
   "source": [
    "# Challenge 2\n",
    "# ---\n",
    "# Question: A cancer medical reasearch institution would like to make predictions on two different \n",
    "# cancer types benign and malignant. Build a model to predict the breast cancer type \n",
    "# (0 = benign or 1 = malignant) given the following dataset. In addition, make a prediction.\n",
    "# NB: Remember to record your observations.\n",
    "# ---\n",
    "# Dataset url = http://bit.ly/BreastCancersDataset\n",
    "# ---\n",
    "# OUR CODE GOES BELOW\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eiWteB6fHPmU"
   },
   "source": [
    "###<font color=\"green\">Challenge 3</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YOZ25wbCHQPt"
   },
   "outputs": [],
   "source": [
    "# Challenge 3\n",
    "# ---\n",
    "# Question: Build a classifier to predict car sales and check the accuracy of the prediction.\n",
    "# given the following dataset\n",
    "# ---\n",
    "# Dataset url = https://bit.ly/3dvU2BB\n",
    "# ---\n",
    "# OUR CODE GOES BELOW\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BA_HMQbPHWQJ",
    "SPpfqZYzrKvs",
    "eagUqvVmeRTx",
    "Pz6qTiItChtg",
    "G8NJS6N6eU7j",
    "wBSYp3fVIupH",
    "uebT2koxr4V4",
    "B-eE6btO3KKM",
    "eiWteB6fHPmU"
   ],
   "name": "AfterWork Data Science: Classification Analysis with Python",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
